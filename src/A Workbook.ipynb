{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dc5167b",
   "metadata": {},
   "source": [
    "# Create DB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c85bc8",
   "metadata": {},
   "source": [
    "In order to integrate with the API and for the purposes of code modularity, we're using SQLAlchemy here. The process of learning how to set up a database has been enlightening, and I'm motivated to continue learning about databases; both working in concert with APIs; and as a means of producing a Data Warehouse of tangible business insights using CRON jobs and ETL.  \n",
    "\n",
    "Whether we use a sqlite database or a postgres one, SQLAlchemy provides an abstraction layer which will make our code more modular should we decide to migrate to a new tech stack. \n",
    "\n",
    "For this project I'll be using a sqlite3 database, to focus more on local ETL processes and analytics/modelling. Since the data comes in a form which could be made into a relational database (some tables missing primary keys), I've decided to showcase my continued curiosity and interest in managing data; from ingestion, through processing, to a reader-friendly output which can answer business questions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d775b9d",
   "metadata": {},
   "source": [
    "In a previous notebook on dynamic programming and algorithms, I noticed that passing the python built-in list() as a default argument (for a function) resulted in all calls to that function utilizing the same list (in the functions definition) within the same instance of the interpreter/notebook.\n",
    "\n",
    "The Base class (generated from declarative_base()) is the parent of all tables we'll create for this project, and holds the shared metadata from which they will each inherit and map their relations. \n",
    "\n",
    "When our table models have been declared (in the models.py file), we can create_all() models, and bind them to the engine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "060fd36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['campaign_desc', 'campaign_table', 'causal_data', 'coupon', 'coupon_redempt', 'hh_demographic', 'product', 'transaction_data']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\polan\\AppData\\Local\\Temp\\ipykernel_5080\\1884740555.py:33: SADeprecationWarning: The Engine.table_names() method is deprecated and will be removed in a future release.  Please refer to Inspector.get_table_names(). (deprecated since: 1.4)\n",
      "  print(engine.table_names())\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker, Session\n",
    "import pandas as pd\n",
    "from fastapi import Depends\n",
    "import models\n",
    "from my_url import _SQLALCHEMY_DATABASE_URL\n",
    "from table_loader import TableLoader\n",
    "#from database import Base\n",
    "\n",
    "\n",
    "### creating a database\n",
    "SQLALCHEMY_DATABASE_URL = _SQLALCHEMY_DATABASE_URL\n",
    "\n",
    "# this engine has a special 'check_same_thread' argument for sqlite3\n",
    "# engine = create_engine(\n",
    "#     SQLALCHEMY_DATABASE_URL, connect_args={\"check_same_thread\": False}\n",
    "# )\n",
    "engine = create_engine(\n",
    "    SQLALCHEMY_DATABASE_URL)\n",
    "\n",
    "# the sessionmaker function creates a new Session for our database interactions, and is the preferred method of transacting with the db when using SQLAlchemy\n",
    "SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n",
    "\n",
    "\n",
    "# this Base is the parent class of our Tables; they can be found in models.py. \n",
    "Base = declarative_base()\n",
    "\n",
    "# # when we create all of the Table models from within models.py, each child class will have an equivalent metadata object (shared by the parent)\n",
    "models.Base.metadata.create_all(bind=engine)\n",
    "\n",
    "\n",
    "print(engine.table_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89601570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for table_name in a.table_names:\n",
    "#     db=next(get_db())\n",
    "#     db.execute(f'DROP TABLE {table_name} CASCADE')\n",
    "#     db.commit()\n",
    "#     print(table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38c48761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'campaign_desc': 0,\n",
      " 'campaign_table': 0,\n",
      " 'causal_data': 0,\n",
      " 'coupon': 0,\n",
      " 'coupon_redempt': 0,\n",
      " 'hh_demographic': 0,\n",
      " 'product': 0,\n",
      " 'transaction_data': 0}\n",
      "campaign_desc\n",
      "campaign_table\n",
      "causal_data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed 0000\n",
      "coupon\n",
      "coupon_redempt\n",
      "hh_demographic\n",
      "failed (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint \"hh_demographic_household_key_key\"\n",
      "DETAIL:  Key (household_key)=(1) already exists.\n",
      "\n",
      "[SQL: INSERT INTO hh_demographic (age_desc, hh_comp_desc, homeowner_desc, household_key, household_size_desc, income_desc, kid_category_desc, marital_status_code) VALUES (%(age_desc)s, %(hh_comp_desc)s, %(homeowner_desc)s, %(household_key)s, %(household_size_desc)s, %(income_desc)s, %(kid_category_desc)s, %(marital_status_code)s) RETURNING hh_demographic.index]\n",
      "[parameters: {'age_desc': '65+', 'hh_comp_desc': '2 Adults No Kids', 'homeowner_desc': 'Homeowner', 'household_key': '1', 'household_size_desc': '2', 'income_desc': '35-49K', 'kid_category_desc': 'None/Unknown', 'marital_status_code': 'A'}]\n",
      "(Background on this error at: https://sqlalche.me/e/14/gkpj)\n",
      "product\n",
      "failed (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint \"product_product_id_key\"\n",
      "DETAIL:  Key (product_id)=(25671) already exists.\n",
      "\n",
      "[SQL: INSERT INTO product (product_id, manufacturer, department, brand, commodity_desc, sub_commodity_desc, curr_size_of_product) VALUES (%(product_id)s, %(manufacturer)s, %(department)s, %(brand)s, %(commodity_desc)s, %(sub_commodity_desc)s, %(curr_size_of_product)s) RETURNING product.index]\n",
      "[parameters: {'product_id': '25671', 'manufacturer': '2', 'department': 'GROCERY', 'brand': 'National', 'commodity_desc': 'FRZN ICE', 'sub_commodity_desc': 'ICE - CRUSHED/CUBED', 'curr_size_of_product': '22 LB'}]\n",
      "(Background on this error at: https://sqlalche.me/e/14/gkpj)\n",
      "transaction_data\n",
      "row 0\r"
     ]
    }
   ],
   "source": [
    "a = TableLoader()\n",
    "import csv \n",
    "chunksize = 1000000\n",
    "def get_db():\n",
    "    db = SessionLocal()\n",
    "    try:\n",
    "        yield db\n",
    "    finally:\n",
    "        db.close()\n",
    "\n",
    "\n",
    "for table_name in a.table_names:\n",
    "    # db.execute(f'DELETE FROM {table_name}')\n",
    "    print(table_name)\n",
    "    try:\n",
    "        with open('../data/'+table_name+'.csv') as f:\n",
    "            reader = csv.reader(f)\n",
    "            fields = [x.casefold() for x in next(reader)]\n",
    "            chunk = []\n",
    "            for idx, row in enumerate(reader):\n",
    "                obj = a.name_model_map[table_name](**dict(zip(fields, row)))\n",
    "                chunk.append(obj)\n",
    "\n",
    "                if idx % chunksize == 0:\n",
    "                    print(f'row {idx}', end='\\r')\n",
    "                    db=next(get_db())\n",
    "\n",
    "                    db.add_all(chunk)\n",
    "                    db.commit()\n",
    "                    chunk.clear()\n",
    "\n",
    "    except StopIteration:  \n",
    "            db=next(get_db())      \n",
    "            db.add_all(chunk)\n",
    "            db.commit()   \n",
    "\n",
    "    \n",
    "    except BaseException as e:\n",
    "        print('failed', e)\n",
    "        db.rollback()\n",
    "        \n",
    "    finally:\n",
    "        db.close()\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14f68de",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = SessionLocal().query(models.TransactionData).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe5b167",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Session' object has no attribute 'Table'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\polan\\Desktop\\2023 - DunnHumby\\src\\A Workbook.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/polan/Desktop/2023%20-%20DunnHumby/src/A%20Workbook.ipynb#Y144sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m SessionLocal()\u001b[39m.\u001b[39;49mTable(\u001b[39m'\u001b[39m\u001b[39mTransactionData\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Session' object has no attribute 'Table'"
     ]
    }
   ],
   "source": [
    "SessionLocal()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0768d7",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "    note: within the *schemas.py* file we can find schemas, which inherit from 'pydantic.BaseModel'. \n",
    "    These are for use with fastAPI, and not to be confused with the SQLAlchemy database abstraction.\n",
    "\n",
    "    Those pydantic dataclass models define some additional parameters which might be necessary in a development database; for example for specific types of transactions. Account creation information like passwords or emails; or additional, mapped information which might be available in the database already for example previous purchases, or queries from an altogether new database (\"gold\" data tables).\n",
    "\n",
    "    These schemas add an additional level of complexity on top of our basic database, but they integrate well with my tech stack, and I believe they offer a significant upside in terms of data accessibility. \n",
    "    \n",
    "    By running some sort of local data API; or automating weekly reports of accounts performance\n",
    "    an API such as this one could serve a variety of data analytics and reporting use-cases. This automation could save time and money for any company requiring data analytics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1888f78",
   "metadata": {},
   "source": [
    "# CRUD UTILITIES\n",
    "\n",
    "\n",
    "Below we define some basic 'CRUD'-style procedures for our local database. First and foremost, we define a dependency -- this wrapper around our sessionmaker() call which we populated as SessionLocal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444a93e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# a wrapper to instantiate a session allows us to ensure the database connection is closed after our transaction; that a new Session is generated each time we interact with the db.\n",
    "# Dependency\n",
    "def get_db():\n",
    "    db = SessionLocal()\n",
    "    try:\n",
    "        yield db\n",
    "    finally:\n",
    "        db.close()\n",
    "\n",
    "\n",
    "# abstracted data write\n",
    "def post_data(model,\n",
    "              db:Session):\n",
    "    db.add(model)\n",
    "    db.commit()\n",
    "    db.refresh(model)\n",
    "    return model\n",
    "\n",
    "def read_table(model: models.Base,\n",
    "               db: Session = Depends(get_db)):\n",
    "    return db.query(models.model).all()\n",
    "\n",
    "def delete_row(model_type,\n",
    "               id_col,\n",
    "               id,\n",
    "              db: Session):\n",
    "    db.query(models.model_type).filter(models.model_type.id_col==id).delete()\n",
    "    db.commit()\n",
    "\n",
    "\n",
    "\n",
    "def write_transactions(db:Session):\n",
    "    df = pd.read_csv('../data/transaction_data.csv')\n",
    "    fields = [x.casefold() for x in df.columns]\n",
    "    for row in df.values:\n",
    "        data = dict(zip(fields, row))\n",
    "        obj = models.TransactionData(**data)\n",
    "        post_data(model=obj, db=db)\n",
    "        #print(data)\n",
    "\n",
    "\n",
    "    # To abstract the local --> db data ingestion step, we could map the Models to their respective file locations...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450f3d40",
   "metadata": {},
   "source": [
    "The cell below runs a check on the tables in our database, dunnhumby.db. In this case, we're going to store the index (skip/offset) for each data source."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0c4e59",
   "metadata": {},
   "source": [
    "# HELPER FUNCTIONS\n",
    "\n",
    "For the purposes of inspecting the database... \n",
    "\n",
    "1. Existing Data Index (allows for restart on data ingestion fail or sequential data ingestion)\n",
    "\n",
    "    - by finding the existing rowcount/index, we can define a skip/offset to any new CRUD operations, be they through the API or locally.\n",
    "        - Considerations related to the 'front-end'; be it an App, a customer-facing website, or an internal API would define how our data gets to us.\n",
    "\n",
    "        - The steps I want to focus on are the creation of new, clean tables -- the exploratory data analysis and required ETL transactions -- and so we'll move on quickly past the data ingestion step.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09138dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "campaign_desc\n",
      "30\n",
      "campaign_table\n",
      "0\n",
      "causal_data\n",
      "0\n",
      "coupon\n",
      "0\n",
      "coupon_redempt\n",
      "0\n",
      "hh_demographic\n",
      "0\n",
      "product\n",
      "0\n",
      "transaction_data\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "tables = ['campaign_desc',\n",
    "'campaign_table',\n",
    "'causal_data',\n",
    "'coupon',\n",
    "'coupon_redempt',\n",
    "'hh_demographic',\n",
    "'product',\n",
    "'transaction_data']\n",
    "\n",
    "con = engine.raw_connection()\n",
    "cursor = con.cursor()\n",
    "\n",
    "existing_rowcounts = dict()\n",
    "\n",
    "for x in tables:\n",
    "    cursor.execute(f'select count(1) from {x}')\n",
    "    # cursor.execute(f'drop table {x}')\n",
    "    print(x)\n",
    "    res = cursor.fetchall()\n",
    "    existing_rowcounts[x] = res[0][0]\n",
    "    print(res[0][0])\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e178b1",
   "metadata": {},
   "source": [
    "Now that we've instantiated our connection to the database and are satisfied with the status of our tables (, pydantic schemas,) and models, we can examine the task of data ingestion from a raw data source. \n",
    "\n",
    "The functions below describe some templates for crud activities from within the database/SQLAlchemy.\n",
    "\n",
    "I noticed that postgresql seems to have a better bulk insert methodology than sqlite3; but depending on the tech stack/use case, these database migration procedures could vary widely. \n",
    "\n",
    "Since this is not, in fact, streaming data, I've opted for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a26719",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#### local database transaction functions\n",
    "def read_hh(db: Session = Depends(get_db)):\n",
    "    return db.query(models.HHDemographic).all()\n",
    "\n",
    "def post_hh(hh_object:models.HHDemographic,\n",
    "            db: Session = Depends(get_db)):\n",
    "    db.add(hh_object)\n",
    "    db.commit()\n",
    "    db.refresh(hh_object)\n",
    "    return hh_object\n",
    "\n",
    "def delete_hh(hh_id\n",
    "              ,db: Session):\n",
    "    # try:\n",
    "    db.query(models.HHDemographic).filter(models.HHDemographic.household_key==hh_id).delete()\n",
    "    db.commit()\n",
    "    # except:\n",
    "    #     print('that household id was not found')\n",
    "\n",
    "\n",
    "def write_demo(db:SessionLocal=SessionLocal()\n",
    "               ,):\n",
    "    df = pd.read_csv('../data/hh_demographic.csv')\n",
    "    fields = [x.casefold() for x in df.columns]\n",
    "    for x in df.values:\n",
    "        post_hh(hh_object=models.HHDemographic(**dict(zip(fields, x))), db=db) \n",
    "\n",
    "\n",
    "\n",
    "def extract_table(table_source_path\n",
    "        ,db:SessionLocal=SessionLocal()\n",
    "               ):\n",
    "    \n",
    "    # in this case use pandas...\n",
    "    df = pd.read_csv(table_source_path)\n",
    "    fields = [x.casefold() for x in df.columns]\n",
    "    for x in df.values:\n",
    "        hh_object=models.HHDemographic(**dict(zip(fields, x)))\n",
    "        db.add(hh_object)\n",
    "        db.commit()\n",
    "        db.refresh(hh_object)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c887e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "class TableLoader:\n",
    "    \"\"\"simulated cron job processing class.\n",
    "        depends on models.py file being loaded with necessary table models present\n",
    "    - accepts:\n",
    "        - data folder filepath\n",
    "                - todo: some sort of qualification of which tables to \"update\"\n",
    "        \n",
    "        - could incorporate logging tools of your choice?   \n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 data_source_prefix:str=\"../data/\",\n",
    "                 skip:int = 0,\n",
    "                 offset:int = 0,\n",
    "                 db:Session = SessionLocal()):\n",
    "\n",
    "        # self.skip = 0\n",
    "        # self.offset = 0\n",
    "        self.chunksize = 10**7\n",
    "        self.db=db\n",
    "        self.data_folder = data_source_prefix\n",
    "\n",
    "        ### specific data sources/endpoints using your prefix; source URL (or disk data)\n",
    "        self.table_names = ['campaign_desc',\n",
    "                        'campaign_table',\n",
    "                        'causal_data',\n",
    "                        'coupon',\n",
    "                        'coupon_redempt',\n",
    "                        'hh_demographic',\n",
    "                        'product',\n",
    "                        'transaction_data']\n",
    "\n",
    "        ### along with a map of your table models (the abstraction layer of SQLALchemy)\n",
    "        self.table_models = [models.CampaignDesc,\n",
    "                            models.CampaignTable,\n",
    "                            models.CausalData,\n",
    "                            models.Coupon,\n",
    "                            models.CouponRedempt,\n",
    "                            models.HHDemographic,\n",
    "                            models.Product,\n",
    "                            models.TransactionData]\n",
    "\n",
    "        ### map the two together for reference\n",
    "        self.name_model_map = dict(zip(self.table_names, self.table_models)) \n",
    "        self.existing_rowcounts = dict()\n",
    "\n",
    "        ### instantiate logging...\n",
    "        self._log = \"\"\n",
    "\n",
    "        ### ping db to find existing rowcount/index for known tables\n",
    "        self.update_existing_rowcounts()\n",
    "        \n",
    "        # run the auto-updating feature to ensure all rows are accounted for...\n",
    "        # for x in self.table_names:\n",
    "        #     self.run_update(x)\n",
    "\n",
    "\n",
    "\n",
    "    def get_existing_rowcount(self, table_name):\n",
    "        con = engine.raw_connection()\n",
    "        cursor = con.cursor()\n",
    "        cursor.execute(f'select count(1) from {table_name}')\n",
    "        res = cursor.fetchall()\n",
    "        return res[0][0]\n",
    "        #self.existing_rowcounts[x] = res[0][0]\n",
    "\n",
    "\n",
    "    def update_existing_rowcounts(self):\n",
    "        con = engine.raw_connection()\n",
    "        cursor = con.cursor()\n",
    "        for x in self.table_names:\n",
    "            self.existing_rowcounts[x] = self.get_existing_rowcount(x)\n",
    "\n",
    "\n",
    "    def delete_known_table(self, table_name):\n",
    "        con = engine.raw_connection()\n",
    "        cursor = con.cursor()\n",
    "        cursor.execute(f'drop table {table_name}')\n",
    "    \n",
    "\n",
    "    def delete_known_tables(self):\n",
    "        for x in self.table_names:\n",
    "            self.delete_known_table(x)\n",
    "\n",
    "\n",
    "    ### logger\n",
    "    @property\n",
    "    def log(self):\n",
    "        return self._log\n",
    "    \n",
    "    @log.setter\n",
    "    def log(self, new):\n",
    "        print(new, flush=True)\n",
    "        self._log += \"\\n \" + new\n",
    "        \n",
    "\n",
    "    def print_log(self):\n",
    "        pprint.pprint(self.log)\n",
    "\n",
    "\n",
    "    #### chunker\n",
    "    # def start_stop(self, offset=0, limit=10**7, chunk_size=10**7):\n",
    "    #     start=offset\n",
    "    #     stop=limit\n",
    "    #     while start < limit:\n",
    "    #         try: \n",
    "    #             yield start, stop\n",
    "    #         except BaseException as e:\n",
    "    #             self.log = f\"Chunker failed with {e}\"\n",
    "    #         finally:\n",
    "    #             start = stop\n",
    "    #             stop = stop + chunk_size\n",
    "\n",
    "    #### clunker\n",
    "    # def update_table(self, table_name):\n",
    "    #     '''cron job'''\n",
    "\n",
    "    #     #### start index\n",
    "    #     start_index = self.existing_rowcounts[table_name] ### the offset\n",
    "\n",
    "    #     #### EXTRACT DATA \n",
    "    #     df = pd.read_csv(self.data_folder+table_name+'.csv').reset_index().set_index('index')\n",
    "    #     if start_index == 0:\n",
    "    #         df.to_sql(name =table_name, con=engine, chunksize=5000000, if_exists='append', method='multi')\n",
    "\n",
    "\n",
    "    #     else:\n",
    "            \n",
    "    #         fields = [x.casefold() for x in df.columns]\n",
    "    #         stop_index = df.shape[0] ### assume this value is the limit (length) of the data we need to pull from the .csv.\n",
    "            \n",
    "\n",
    "    #         #### DATA VERIFICATION ASSERTIONS...ADD REAL INDEX COMPARISON?\n",
    "    #         assert start_index < stop_index, f'start {start_index} < {stop_index} stop'\n",
    "\n",
    "    #         self.log = f\"Beginning '{table_name}' Update...\"\n",
    "    #         self.log = f\"Starting at 0-index {start_index}, going up to but not including {stop_index}...\"\n",
    "    #         self.log = f\"Shape is {df.shape}. Fields are {fields}...\"\n",
    "            \n",
    "    #         rows_to_rip = stop_index - start_index\n",
    "    #         try:\n",
    "    #             #### automatic check for filesize --> do we need chunking?\n",
    "    #             if rows_to_rip < 10**7:\n",
    "    #                 self.log = f\"no chunking necessary...\"\n",
    "    #                 #### regular row-level upload?\n",
    "    #                 for x in df.values[start_index:stop_index]:\n",
    "    #                     hh_object=self.name_model_map[table_name](**dict(zip(fields, x)))\n",
    "    #                     self.db.add(hh_object)\n",
    "    #                     self.db.commit()\n",
    "    #                     self.db.refresh(hh_object)\n",
    "    #             else:\n",
    "    #                 #### enter chunking logic\n",
    "    #                 self.log = f'entering chunking logic;'\n",
    "    #                 start_stopper = self.start_stop(offset=start_index\n",
    "    #                                                 , limit=stop_index\n",
    "    #                                                 , chunk_size=10**7)\n",
    "\n",
    "    #                 while start_index <= stop_index:\n",
    "    #                     start_index, stop_index = next(start_stopper)\n",
    "    #                     self.log = f'rows {start_index} through {stop_index} of {len(df.values)}'\n",
    "    #                     chunk = list(df.values[start_index:stop_index])\n",
    "\n",
    "    #                     self.db.add_all([self.name_model_map[table_name](**dict(zip(fields, x))) for x in chunk])\n",
    "    #                     self.db.commit()\n",
    "\n",
    "    #         except BaseException as e:\n",
    "    #             #print(f'process failed on table {file}: {e}')\n",
    "    #             self.log += f'process failed on table {table_name}: {e}'\n",
    "\n",
    "    \n",
    "    def insert_table(self, table_name):\n",
    "        \"\"\"for our use case, we just need the whole data to go in.\n",
    "         only use this if the table models have been created, but the table is empty \"\"\"\n",
    "        \n",
    "        try:\n",
    "            self.log = f\" inserting {x}\"\n",
    "            assert self.existing_rowcounts[table_name] == 0, f'{table_name} already has data populated. Use update_table'\n",
    "            df = pd.read_csv(self.data_folder+table_name+'.csv')\n",
    "            df.to_sql(name =table_name, con=engine, chunksize=500000, if_exists='append', method='multi')\n",
    "        except AssertionError as e:\n",
    "            self.log = str(e)\n",
    "\n",
    "\n",
    "    def insert_all(self, all_names:list=None):\n",
    "        if all_names == None:\n",
    "            all_names = self.table_names\n",
    "        for x in all_names:\n",
    "            self.log = f'reading table {x}'\n",
    "            try:\n",
    "                self.insert_table(x)\n",
    "            except:\n",
    "                continue\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b647569e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = TableLoader(db=SessionLocal())\n",
    "\n",
    "# for x in a.table_names:\n",
    "#     if x not in  ['causal_data', 'coupon', 'transaction_data', 'product']:\n",
    "#         try:\n",
    "#             a.run_update(x)\n",
    "#         except AssertionError as e:\n",
    "#             print(x, \"ok\", e)\n",
    "#             continue\n",
    "# first_log = a.log\n",
    "\n",
    "# b = TableLoader(db=SessionLocal())\n",
    "\n",
    "# for x in b.table_names:\n",
    "#     try:\n",
    "#         a.run_update(x)\n",
    "#     except AssertionError as e:\n",
    "#         print(x, \"ok\", e)\n",
    "#         continue\n",
    "\n",
    "# second_log = a.log\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adc1a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " inserting transaction_data\n",
      "campaign_desc already has data populated. Use update_table\n",
      " inserting transaction_data\n",
      "campaign_table already has data populated. Use update_table\n",
      " inserting transaction_data\n",
      " inserting transaction_data\n",
      " inserting transaction_data\n",
      "coupon_redempt already has data populated. Use update_table\n",
      " inserting transaction_data\n",
      "hh_demographic already has data populated. Use update_table\n",
      " inserting transaction_data\n",
      " inserting transaction_data\n"
     ]
    }
   ],
   "source": [
    "a.insert_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b2a5a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\\n'\n",
      " '  inserting transaction_data\\n'\n",
      " ' campaign_desc already has data populated. Use update_table\\n'\n",
      " '  inserting transaction_data\\n'\n",
      " ' campaign_table already has data populated. Use update_table\\n'\n",
      " '  inserting transaction_data\\n'\n",
      " '  inserting transaction_data\\n'\n",
      " '  inserting transaction_data\\n'\n",
      " ' coupon_redempt already has data populated. Use update_table\\n'\n",
      " '  inserting transaction_data\\n'\n",
      " ' hh_demographic already has data populated. Use update_table\\n'\n",
      " '  inserting transaction_data\\n'\n",
      " '  inserting transaction_data')\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(a.log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec72805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.get_existing_rowcount('campaign_desc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5416e2d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n  inserting transaction_data\\n campaign_desc already has data populated. Use update_table\\n  inserting transaction_data\\n campaign_table already has data populated. Use update_table\\n  inserting transaction_data\\n  inserting transaction_data\\n  inserting transaction_data\\n coupon_redempt already has data populated. Use update_table\\n  inserting transaction_data\\n hh_demographic already has data populated. Use update_table\\n  inserting transaction_data\\n  inserting transaction_data'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9808eda2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\\n'\n",
      " '  inserting transaction_data\\n'\n",
      " ' campaign_desc already has data populated. Use update_table\\n'\n",
      " '  inserting transaction_data\\n'\n",
      " ' campaign_table already has data populated. Use update_table\\n'\n",
      " '  inserting transaction_data\\n'\n",
      " '  inserting transaction_data\\n'\n",
      " '  inserting transaction_data\\n'\n",
      " ' coupon_redempt already has data populated. Use update_table\\n'\n",
      " '  inserting transaction_data\\n'\n",
      " ' hh_demographic already has data populated. Use update_table\\n'\n",
      " '  inserting transaction_data\\n'\n",
      " '  inserting transaction_data')\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pprint.pprint(a.log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f9805d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TableLoader' object has no attribute 'run_update'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\polan\\Desktop\\2023 - DunnHumby\\src\\Data Ingestion.ipynb Cell 22\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/polan/Desktop/2023%20-%20DunnHumby/src/Data%20Ingestion.ipynb#X30sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m a\u001b[39m.\u001b[39;49mrun_update()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'TableLoader' object has no attribute 'run_update'"
     ]
    }
   ],
   "source": [
    "a.run_update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dc0f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resulting_objects = SessionLocal().query(models.HHDemographic).all()\n",
    "# len(resulting_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340bc3ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbb043a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5e08e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.run_update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24960bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint \n",
    "pprint.pprint(a.log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dc6d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/causal_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee353485",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca9c5cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebb62bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95dfcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # print(x)\n",
    "    # df = pd.read_csv(f'{filepath}{x}'+\".csv\")\n",
    "    # df.columns = [x.casefold() for x in df.columns]\n",
    "    # df.reset_index(inplace=True)\n",
    "    # df.to_sql(x, con=engine, if_exists='append')\n",
    "    # print(df.info())\n",
    "\n",
    "        # for row in df.values:\n",
    "        #     #\n",
    "        #     data = dict(zip(fields, row))\n",
    "        #     # create row object with values\n",
    "        #     obj = name_model_map[x](**data)\n",
    "        #     # \n",
    "        #     post_data(model=obj, db=db)\n",
    "    \n",
    "    # ### verify rowcount\n",
    "    # con = engine.raw_connection()\n",
    "    # cursor = con.cursor()\n",
    "    # cursor.execute(f'select count(1) from {x}')\n",
    "    # res = cursor.fetchall()\n",
    "    # print(f'{res[0][0]} rows loaded into Table {x}')\n",
    "    # #assert res[0][0] == df.shape[0]\n",
    "\n",
    "\n",
    "    # ### verify column names? note that some models have a new, generated index/primary key column...\n",
    "    # con = engine.raw_connection()\n",
    "    # cursor = con.cursor()\n",
    "    # cursor.execute(f'select * from {x} limit 1')\n",
    "    # res = cursor.fetchall()\n",
    "    # print(f'{len(res)} columns loaded to Table {x}')\n",
    "    # print(df.shape)\n",
    "        #assert len(res[0]) == df.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085f3079",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33be7f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_hh(next(get_db()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d933395",
   "metadata": {},
   "source": [
    "SQLAlchemy recommends using its engine to import data. In this method we try a shortcut. After establishing our engine, and binding the Parent class for our table models using the models.Base.metadata.create_all(bind=engine) call, we've instantiated the 'image' of our row instances into the database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72141dea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624e1894",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(get_db()).query(models.HHDemographic).filter(models.HHDemographic.household_key == 1).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6324790",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_table(models.HHDemographic, db=next(get_db()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a03e14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ba3cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import inspect\n",
    "import models\n",
    "mapper = inspect(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d24c9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba224339",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/campaign_desc.csv')\n",
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6aebe2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de05b86",
   "metadata": {},
   "source": [
    "# Method 1: Importing using pandas.DataFrame.to_sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6a526f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../data/transaction_data.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e508d919",
   "metadata": {},
   "outputs": [],
   "source": [
    "con = engine.raw_connection()\n",
    "cursor = con.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134bcd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_sql(\"transaction_data\", con, if_exists='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b8f9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute('drop table transaction_data')\n",
    "res = cursor.fetchall()\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f93d28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4a1290",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad6a082",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980e288b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69695d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import MetaData\n",
    "metadata_obj = MetaData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbabef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy.schema import MetaData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ef88e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for schema in schemas:\n",
    "#     print(\"schema: %s\" % schema)\n",
    "#     for table_name in inspector.get_table_names(schema=schema):\n",
    "#         for column in inspector.get_columns(table_name, schema=schema):\n",
    "#             print(\"Column: %s\" % column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4153e318",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4593bdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_demo(next(get_db()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98aa2bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "con = sqlite3.connect(\"src/dunnhumby.db\")\n",
    "\n",
    "cur = con.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766161ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking existing tables\n",
    "cur.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "output_tables = [x[0] for x in cur.fetchall()]\n",
    "\n",
    "output_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d50d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in output_tables:\n",
    "    \n",
    "    cur.execute(f\"SELECT count(1) FROM {x}\")\n",
    "    res = cur.fetchall()\n",
    "    print(res)\n",
    "#     cur.execute(f\"DROP TABLE IF EXISTS RAW_{x}\")\n",
    "    cur.execute(f\"SELECT * FROM {x} LIMIT 1;\")\n",
    "    res = cur.fetchall()\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52c3db2",
   "metadata": {},
   "source": [
    "# Writing data to db from .csv files \n",
    "## using pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a4eb0f",
   "metadata": {},
   "source": [
    "glob module to get a list of filenames.\n",
    "\n",
    "pandas built-in function DataFrame.to_sql() using the connection to dunnhumby.db\n",
    "\n",
    "- if_exists='fail' parameter to force an error if the table exists already. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92443081",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in glob.glob('data/*.csv'):\n",
    "    print('*'*50)\n",
    "    filename = file.split(\"\\\\\")[1][:-4]\n",
    "    print(filename)\n",
    "    df = pd.read_csv(file)\n",
    "    #display(df.head(5))\n",
    "    #print(df.info())\n",
    "    df.to_sql(filename, con, index=False)\n",
    "    print(f'{filename} written to database')\n",
    "    print('*'*50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35ed7f2",
   "metadata": {},
   "source": [
    "#   non-pandas data imputation    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb5c22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for file in glob.glob('data/*.csv'):\n",
    "    \n",
    "    \n",
    "    columns = \",\".join(df.columns)\n",
    "    #cur.execute(f\"CREATE TABLE IF NOT EXISTS RAW_{filename}({columns})\")\n",
    "    data = [tuple(x) for x in df.values]\n",
    "    cur.executemany(f\"INSERT INTO {filename} VALUES({', '.join(['?' for x in df.columns])})\", data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46751559",
   "metadata": {},
   "source": [
    "# Checking the tables exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f5f61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking existing tables\n",
    "cur.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "output_tables = [x[0] for x in cur.fetchall()]\n",
    "\n",
    "output_tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a1f50b",
   "metadata": {},
   "source": [
    "# Checking the rowcounts match the .csv rowcounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ffd606",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "cur.execute(f\"SELECT * from hh_demographic where household_key=1\")\n",
    "res = cur.fetchall()\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22717170",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in output_tables:\n",
    "    \n",
    "    cur.execute(f\"SELECT count(1) FROM {x}\")\n",
    "    res = cur.fetchall()\n",
    "    print(res)\n",
    "#     cur.execute(f\"DROP TABLE IF EXISTS RAW_{x}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5677f23a",
   "metadata": {},
   "source": [
    "# Verifying rows look as they should"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2bb1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in output_tables:\n",
    "    cur.execute(f\"SELECT * FROM {x} LIMIT 5;\")\n",
    "    res = cur.fetchall()\n",
    "    display(pd.DataFrame(res))\n",
    "    # note column names don't come back out of the database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e249c9",
   "metadata": {},
   "source": [
    "Despite the coupon tables being stored in bytecode in the db, pandas can read them without issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9baa76d",
   "metadata": {},
   "source": [
    "# Joining Household Data onto aggregate sales data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aeb015b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(f\"SELECT SUM(SALES_VALUE) FROM transaction_data GROUP BY household_key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1d50dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = cur.fetchall()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38df8fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pd.DataFrame(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b184a435",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
